{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Light_LEMONADE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1pD1U_FJDvvTLF6DquJSIAXGABIhGo9CF",
      "authorship_tag": "ABX9TyMQNlvGh9WvvQwnCuuLlE7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MY-Park/Light-LEMONADE/blob/master/Light_LEMONADE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG0Q5VArFv_q",
        "colab_type": "text"
      },
      "source": [
        "## **Light LEMONADE**\n",
        "\n",
        "**L**armarckian **E**volutionary algorithm for **M**ulti-**O**bjective **N**eural **A**rchitecture **DE**sign(**LEMONADE**) [1] 알고리즘을 작은 스케일로 재구현한 **Light LEMONADE** 실험 코드입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suVfuFyOHYJy",
        "colab_type": "text"
      },
      "source": [
        "딥러닝 라이브러리는 tensorflow keras를 사용하였으므로 환경을 세팅해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8P0VtAAxcnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5718bad4-b8a1-4086-dba0-9a03603a6e84"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCvfr4KJHhWJ",
        "colab_type": "text"
      },
      "source": [
        "학습에 사용할 데이터는 Fashion MNIST[2] 로서 다음과 같이 데이터를 다운 받아줍니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnYi1PHFxjO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6166d4fb-dd90-42ef-9fd9-c69b8d7bd8c3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#fasion mnist 데이터셋의 train, test set을 가져옵니다.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# (# train set : 55,000, test set : 5000)\n",
        "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
        "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
        "\n",
        "# 입력 이미지의 크기를 (28, 28) 에서 (28, 28, 1) 로 배열 차원을 변경(reshape)\n",
        "w, h = 28, 28\n",
        "x_train = x_train.reshape(x_train.shape[0], w, h, 1)\n",
        "x_valid = x_valid.reshape(x_valid.shape[0], w, h, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], w, h, 1)\n",
        "\n",
        "# 레이블에 원-핫 인코딩 적용\n",
        "# 원-핫 벡터는 단 하나의 차원에서만 1이고, 나머지 차원에서는 0인 벡터입니다.\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# 학습 셋 크기\n",
        "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
        "\n",
        "# 학습용, 검증용, 테스트용 데이터셋의 개수\n",
        "print(x_train.shape[0], 'train set')\n",
        "print(x_valid.shape[0], 'validation set')\n",
        "print(x_test.shape[0], 'test set')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28) y_train shape: (60000,)\n",
            "x_train shape: (55000, 28, 28, 1) y_train shape: (55000, 10)\n",
            "55000 train set\n",
            "5000 validation set\n",
            "10000 test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK2NvrUwI2kS",
        "colab_type": "text"
      },
      "source": [
        "Light LEMONADE에 사용될 Initial Population을 설정해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3IIZv9bTDZ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bfcf07c3-eeb6-43e0-e5ce-f543ce8db6ab"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "pareto_front = set()\n",
        "\n",
        "#num of filters 4, 8, 16, 32, 64\n",
        "for i in range(2,7):\n",
        "  filter = 2 ** i\n",
        "  inputs1 = tf.keras.layers.Input((28, 28, 1))\n",
        "  x = tf.keras.layers.Conv2D(filter, (3, 3), activation=None, padding='same')(inputs1)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(10, activation=None)(x)\n",
        "  x = tf.keras.layers.Activation('softmax')(x)\n",
        "\n",
        "  model = tf.keras.models.Model(inputs=inputs1, outputs=x)\n",
        "  #model.summary()\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "  model_path = '/content/drive/My Drive/SBSE/model/model_g0_p'+ str(i) + '.h5'\n",
        "  model.save(model_path)\n",
        "\n",
        "  idx=0\n",
        "  ckt_path = '/content/drive/My Drive/SBSE/weight/model.weights.best.g0_p'+str(i)+'.hdf5'\n",
        "  \n",
        "  ## epoch 수를 늘리고 early stopping을 사용하고 싶다면 아래 코드를 활성화 시키고 \n",
        "  ## model.fit의 파라미터로 callbacks = [es, mc]를 사용해줍니다.\n",
        "  #es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)\n",
        "\n",
        "  mc = ModelCheckpoint(filepath=ckt_path, verbose = 0, save_best_only=True)\n",
        "  train_history = model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=64,\n",
        "          epochs=1,\n",
        "          validation_data=(x_valid, y_valid),\n",
        "          callbacks=[mc])\n",
        "\n",
        "  val_loss = min(train_history.history['val_loss'])\n",
        "  num_param = model.count_params()\n",
        "  id = 'g0_p'+str(i)\n",
        "  pareto_front.add((id, num_param, val_loss))\n",
        "\n",
        "pf_copy = pareto_front\n",
        "# save pareto front for each generation \n",
        "pf_path = '/content/drive/My Drive/SBSE/pareto_front/pf_g0.npy'\n",
        "list_pf = list(pf_copy)\n",
        "np.save(pf_path, list_pf)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 13s 15ms/step - loss: 0.4423 - accuracy: 0.8459 - val_loss: 0.3533 - val_accuracy: 0.8792\n",
            "860/860 [==============================] - 3s 4ms/step - loss: 0.4304 - accuracy: 0.8495 - val_loss: 0.3499 - val_accuracy: 0.8776\n",
            "860/860 [==============================] - 3s 4ms/step - loss: 0.4273 - accuracy: 0.8538 - val_loss: 0.3268 - val_accuracy: 0.8824\n",
            "860/860 [==============================] - 4s 4ms/step - loss: 0.4865 - accuracy: 0.8496 - val_loss: 0.3671 - val_accuracy: 0.8734\n",
            "860/860 [==============================] - 4s 5ms/step - loss: 0.5412 - accuracy: 0.8486 - val_loss: 0.4614 - val_accuracy: 0.8550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oClf-QEyJkzE",
        "colab_type": "text"
      },
      "source": [
        "### Network Operator\n",
        "\n",
        "parent network에 operator를 적용하여 child network를 생성하는데, 4가지 operator를 설정해줍니다. \n",
        "1. 임의의 ReLU layer 다음에 Convolution(Conv)-BatchNormalization(BatchNorm)-ReLU 블록을 삽입한다. \n",
        "2. 임의의 Conv layer 다음에 연속하여 Conv layer를 추가한다. \n",
        "3. 임의의 ReLU activation layer output 2개를 골라 skip connection을 추가한다.\n",
        "4. 임의로 선택한 레이어 한 개를 제거한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwBFNue6QmbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "####################\n",
        "# network morphism #\n",
        "####################\n",
        "# 1. add conv-batchnorm-relu block after relu\n",
        "def add_block(o_model):\n",
        "\n",
        "  relu_list = []\n",
        "  relu_cnt = 0\n",
        "\n",
        "  filter = 0\n",
        "  for layer in o_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Activation):\n",
        "      relu_list.append(layer)\n",
        "      relu_cnt += 1\n",
        "    elif isinstance(layer, tf.keras.layers.Conv2D):\n",
        "      filter = layer.output_shape[3]\n",
        "\n",
        "  if (relu_cnt-2 < 0):\n",
        "    new_model = o_model\n",
        "    return new_model\n",
        "\n",
        "  rnd = random.randint(0,relu_cnt-2)\n",
        "  targeted_relu = relu_list[rnd]\n",
        "\n",
        "  len_layers = len(o_model.layers)\n",
        "  x = o_model.layers[0].output\n",
        "\n",
        "  for i in range(1,len_layers):\n",
        "\n",
        "    x = o_model.layers[i](x)\n",
        "\n",
        "    if o_model.layers[i] == targeted_relu:\n",
        "      x = tf.keras.layers.Conv2D(filters=filter, kernel_size=3, padding='same')(x)\n",
        "      x = tf.keras.layers.BatchNormalization()(x)\n",
        "      x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "  new_model = tf.keras.models.Model(o_model.layers[0].input, x)\n",
        "  return new_model\n",
        "\n",
        "# 2. add conv after conv\n",
        "\n",
        "def add_conv(o_model):\n",
        "  conv_list = []\n",
        "  conv_cnt = 0\n",
        "  filter = 0\n",
        "  for layer in o_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "      conv_list.append(layer)\n",
        "      conv_cnt += 1\n",
        "      filter = layer.output_shape[3]\n",
        "\n",
        "  rnd = random.randint(0,conv_cnt-1)\n",
        "  targeted_conv = conv_list[rnd]\n",
        "\n",
        "  len_layers = len(o_model.layers)\n",
        "  x = o_model.layers[0].output\n",
        "\n",
        "  for i in range(1,len_layers):\n",
        "\n",
        "    x = o_model.layers[i](x)\n",
        "    \n",
        "    if o_model.layers[i] == targeted_conv:\n",
        "      # conv with random number of filters\n",
        "      x = tf.keras.layers.Conv2D(filters=filter, kernel_size=3, padding='same')(x)\n",
        "\n",
        "  new_model = tf.keras.models.Model(o_model.layers[0].input, x)\n",
        "  return new_model\n",
        "\n",
        "# 3. add skip connection\n",
        "def add_skip_connection(o_model):\n",
        "  relu_list = []\n",
        "  relu_cnt = 0\n",
        "  for layer in o_model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Activation):\n",
        "      relu_list.append(layer)\n",
        "      relu_cnt += 1\n",
        "\n",
        "  if relu_cnt-1 < 2:\n",
        "    new_model = o_model\n",
        "    return new_model\n",
        "  \n",
        "  rnd_list = random.sample(range(relu_cnt-1), 2)\n",
        "  rnd_list.sort()\n",
        "\n",
        "  input_relu = relu_list[rnd_list[0]]\n",
        "  output_relu = relu_list[rnd_list[1]]\n",
        "\n",
        "  len_layers = len(o_model.layers)\n",
        "  x = o_model.layers[0].output\n",
        "  shortcut = o_model.layers[0].output\n",
        "\n",
        "  for i in range(1,len_layers):\n",
        "    x = o_model.layers[i](x)\n",
        "\n",
        "    if o_model.layers[i] == input_relu:\n",
        "      shortcut = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    elif o_model.layers[i] == output_relu:\n",
        "      x = tf.keras.layers.add([x,shortcut])\n",
        "\n",
        "    \n",
        "  new_model = tf.keras.models.Model(o_model.layers[0].input, x)\n",
        "  return new_model\n",
        "\n",
        "################################\n",
        "# approximate network morphism #\n",
        "################################\n",
        "# 4. remove randomly chosen layer or skip connection\n",
        "\n",
        "def remove_random_layer(o_model):\n",
        "  len_layers = len(o_model.layers)\n",
        "  if (len_layers-4 < 2):\n",
        "    new_model = o_model\n",
        "    return new_model\n",
        "  rnd = random.randint(2,len_layers-4)\n",
        "  new_model = o_model\n",
        "  pop_layer = new_model.layers.pop(rnd)\n",
        "\n",
        "  x = o_model.layers[0].output\n",
        "  for i in range(1, len_layers):\n",
        "    if pop_layer == o_model.layers[i]:\n",
        "      continue\n",
        "    x = o_model.layers[i](x)\n",
        "  new_model = tf.keras.models.Model(o_model.layers[0].input, x)\n",
        "  return new_model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXvfZZnvLdLM",
        "colab_type": "text"
      },
      "source": [
        " 설정한 generation 동안 pareto front를 추정합니다.\n",
        "\n",
        "* from_gen : 몇 generation 부터 학습을 시작할 건지 정해줍니다.\n",
        "* to_gen : 몇 generation까지 학습을 할 것인지 정해줍니다.\n",
        "* num_sample : child sampling할 때 몇개를 sampling할지 정해줍니다. sampling 된 child에 대해서만 expensive object인 validation loss를 계산합니다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPNgQf4_69lL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79425133-d1a1-4eb3-fcac-3b95d187f21e"
      },
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "func_list = [add_block, add_conv, add_skip_connection, remove_random_layer]\n",
        "from_gen = 1\n",
        "to_gen = 2\n",
        "num_sample = 20\n",
        "\n",
        "# for plot pareto front\n",
        "x_list = []\n",
        "y_list = []\n",
        "\n",
        "pf_copy = np.load('/content/drive/My Drive/SBSE/pareto_front/pf_g'+str(from_gen-1)+'.npy')\n",
        "pf_copy = pf_copy.tolist()\n",
        "\n",
        "pareto_front = set()\n",
        "for pf in pf_copy:\n",
        "  id, num, val = pf\n",
        "  pareto_front.add((id, int(num), float(val)))\n",
        "\n",
        "for gen in range(from_gen, to_gen+1):\n",
        "  print('Generation ' + str(gen) + '\\n')\n",
        "  # pool for children\n",
        "  pool = []\n",
        "\n",
        "  num_param_list = []\n",
        "\n",
        "  # o_model : 현재 pareto front인 부모 network\n",
        "  for opt in pareto_front:\n",
        "    id, _, _ = opt\n",
        "    o_model = tf.keras.models.load_model('/content/drive/My Drive/SBSE/model/model_'+id+'.h5')\n",
        "\n",
        "    # generate child network\n",
        "    for func in func_list:\n",
        "      new_model = func(o_model)\n",
        "      pool.append(new_model)\n",
        "      num_param_list.append(new_model.count_params())\n",
        "\n",
        "  ## 3. 가우시안 커널 밀도 추정\n",
        "  estimator = stats.gaussian_kde(num_param_list, bw_method='silverman')\n",
        "\n",
        "  # cheap object에 대한 child 네트워크의 분포와 그에 따른 커널 밀도 추정 결과를 그래프로 출력할 수 있습니다.\n",
        "\n",
        "  # X = np.arange(0, 600000, 10)\n",
        "  # K = estimator(X)\n",
        "  # plt.style.use(['seaborn-ticks'])\n",
        "  # plt.plot(X,K, label='Population density')\n",
        "  # plt.hist(num_param_list, density=True, bins=30, label='Children Probability')\n",
        "  # plt.xlabel('Number of Parameters')\n",
        "  # plt.legend()\n",
        "  # plt.savefig('/content/drive/My Drive/SBSE/density.svg',format='svg',dpi=1200)\n",
        "\n",
        "  break\n",
        "\n",
        "  prob_param_list = []\n",
        "  sum = 0\n",
        "\n",
        "  # 추정된 밀도의 역수의 확률로 child network를 샘플링합니다. 추정된 밀도가 0일 경우를 대비하여 아주 작은 epsilon(1e^-7)을 더해주었습니다.\n",
        "  for num_param in num_param_list:\n",
        "    prob = 1/(estimator(num_param)[0]+0.0000001)\n",
        "    prob_param_list.append(prob)\n",
        "    sum += prob\n",
        "\n",
        "  for i in range(len(prob_param_list)):\n",
        "    prob_param_list[i] /= sum\n",
        "\n",
        "  sample_num = min(len(pool), num_sample)\n",
        "  c_sample = np.random.choice(len(pool), sample_num, replace=False, p=prob_param_list)\n",
        "\n",
        "  loss_list = []\n",
        "  selected_pool = []\n",
        "\n",
        "  # selected children에 대해 expensive objective인 validation loss를 계산합니다.\n",
        "  for c_idx in c_sample:\n",
        "    c_model = pool[c_idx]\n",
        "    c_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    id = 'g'+str(gen)+'_p'+str(c_idx)\n",
        "    \n",
        "    ## epoch을 늘리고 early stopping을 하고 싶을 경우 아래 코드를 활성화합니다. 그리고 fit 파라미터로 callbacks = [mc, es]를 사용하면 됩니다.\n",
        "    #es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "\n",
        "    mc = ModelCheckpoint(filepath='/content/drive/My Drive/SBSE/weight/model.weights.best.'+id+'.hdf5', verbose = 0, save_best_only=True)\n",
        "    train_history = c_model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=64,\n",
        "          epochs=5,\n",
        "          verbose=0,\n",
        "          validation_data=(x_valid, y_valid),\n",
        "          callbacks=[mc])\n",
        "    val_loss = min(train_history.history['val_loss'])\n",
        "\n",
        "    pareto_front.add((id, c_model.count_params(), val_loss))\n",
        "    c_model.save('/content/drive/My Drive/SBSE/model/model_'+id+'.h5')\n",
        "\n",
        "  delete_list=[]\n",
        "\n",
        "  # select pareto front\n",
        "  for pf1 in pareto_front:\n",
        "    # count for dominate i\n",
        "    # 현재 네트워크보다 dominate한 네트워크의 개수를 세서, 그 개수가 0인 네트워크만 pareto front로 남겨둡니다.\n",
        "    cnt = 0\n",
        "    _, num_param1, val_loss1 = pf1\n",
        "    num_param1\n",
        "    for pf2 in pareto_front:\n",
        "      if pf1 == pf2 :\n",
        "        continue\n",
        "        print(pf1,pf2)\n",
        "      _, num_param2, val_loss2 = pf2\n",
        "      if ((num_param1 > num_param2) & (val_loss1 > val_loss2)):\n",
        "        cnt += 1\n",
        "      elif ((num_param1 >= num_param2) & (val_loss1 > val_loss2)):\n",
        "        cnt += 1\n",
        "      elif ((num_param1 > num_param2) & (val_loss1 >= val_loss2)):\n",
        "        cnt += 1\n",
        "\n",
        "      # 더 이상 pareto front가 아닌 네트워크는 population에서 제거해줍니다.     \n",
        "      if cnt > 0:\n",
        "        delete_list.append(pf1)\n",
        "        break\n",
        "  \n",
        "  for pf in delete_list:\n",
        "    pareto_front.remove(pf)\n",
        "\n",
        "  x_list.append([loss for _,_,loss in pareto_front])\n",
        "  y_list.append([num_param for _, num_param, _ in pareto_front])\n",
        "\n",
        "  pf_copy = pareto_front\n",
        "  # save pareto front for each generation \n",
        "  pf_path = '/content/drive/My Drive/SBSE/pareto_front/pf_g' + str(gen) +'.npy'\n",
        "  list_pf = list(pf_copy)\n",
        "  np.save(pf_path, list_pf)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation 1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_oP4LWmQ_Ut",
        "colab_type": "text"
      },
      "source": [
        "## 결과\n",
        "추정된 pareto front에 대한 정보를 출력해봅니다.\n",
        "\n",
        "(pareto front 의 id, parameter 개수, validation loss)를 출력하고,\n",
        "\n",
        "각 모델에 대한 test loss, test accuracy를 출력해봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAfr_YurW4KK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "354601dc-5ba9-438b-b442-9401beb16853"
      },
      "source": [
        "gen = 1\n",
        "pf_path = '/content/drive/My Drive/SBSE/pareto_front/pf_g' + str(gen) +'.npy'\n",
        "\n",
        "pareto_front = np.load(pf_path)\n",
        "print(pareto_front)\n",
        "\n",
        "for id,_,_ in pareto_front:\n",
        "  path = '/content/drive/My Drive/SBSE/model/model_'+id+'.h5'\n",
        "  model = tf.keras.models.load_model(path)\n",
        "  score = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['g1_p15' '62810' '0.3528299033641815']\n",
            " ['g1_p14' '62842' '0.3322029113769531']\n",
            " ['g1_p4' '128058' '0.3179399371147156']\n",
            " ['g1_p18' '251338' '0.3070986568927765']\n",
            " ['g0_p4' '125674' '0.31878188252449036']\n",
            " ['g1_p1' '31574' '0.3563233017921448']\n",
            " ['g0_p2' '31426' '0.35767918825149536']\n",
            " ['g1_p12' '63458' '0.32881060242652893']\n",
            " ['g1_p5' '127994' '0.3180311322212219']]\n",
            "Test loss: 0.3779846727848053\n",
            "Test accuracy: 0.8669000267982483\n",
            "Test loss: 0.3535482585430145\n",
            "Test accuracy: 0.8765000104904175\n",
            "Test loss: 0.3320382237434387\n",
            "Test accuracy: 0.8914999961853027\n",
            "Test loss: 0.32459357380867004\n",
            "Test accuracy: 0.8873999714851379\n",
            "Test loss: 2.328130006790161\n",
            "Test accuracy: 0.09309999644756317\n",
            "Test loss: 0.38554516434669495\n",
            "Test accuracy: 0.8590999841690063\n",
            "Test loss: 2.536911725997925\n",
            "Test accuracy: 0.0828000009059906\n",
            "Test loss: 0.3501076400279999\n",
            "Test accuracy: 0.8730999827384949\n",
            "Test loss: 0.3328883647918701\n",
            "Test accuracy: 0.8870999813079834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su_PFkSdRtDJ",
        "colab_type": "text"
      },
      "source": [
        "pareto front를 구성하고 있는 뉴럴 네트워크의 구조를 살펴봅니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfI4hRp_vc_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ceb9a06-52aa-46c2-dbb3-13d906ed1cdb"
      },
      "source": [
        "for id,_,_ in pareto_front:\n",
        "    model = tf.keras.models.load_model('/content/drive/My Drive/SBSE/model/model_'+id+'.h5')\n",
        "    model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_93\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                62730     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 62,810\n",
            "Trainable params: 62,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                62730     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 62,842\n",
            "Trainable params: 62,826\n",
            "Non-trainable params: 16\n",
            "_________________________________________________________________\n",
            "Model: \"functional_77\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                125450    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 128,058\n",
            "Trainable params: 127,994\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                250890    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 251,338\n",
            "Trainable params: 251,274\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n",
            "Model: \"functional_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                125450    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 125,674\n",
            "Trainable params: 125,642\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n",
            "Model: \"functional_73\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 4)         40        \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 28, 28, 4)         148       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 4)         16        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 4)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                31370     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 31,574\n",
            "Trainable params: 31,566\n",
            "Non-trainable params: 8\n",
            "_________________________________________________________________\n",
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 4)         40        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 4)         16        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 28, 4)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                31370     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 31,426\n",
            "Trainable params: 31,418\n",
            "Non-trainable params: 8\n",
            "_________________________________________________________________\n",
            "Model: \"functional_89\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 28, 28, 8)         584       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 28, 28, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 28, 28, 8)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                62730     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 63,458\n",
            "Trainable params: 63,426\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n",
            "Model: \"functional_79\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 28, 28, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                125450    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 127,994\n",
            "Trainable params: 127,962\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67LDU8N8NEUc",
        "colab_type": "text"
      },
      "source": [
        "매 generation마다 Pareto Front의 추정 과정을 그래프로 출력해줍니다.\n",
        "\n",
        "출력하고 싶은 generation을 pareto_list에 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62Yx7MnWkItN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "4f9af1a2-b2ec-481a-c3f1-493fde36b30a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mlp\n",
        "import numpy as np\n",
        "import matplotlib.transforms as mtrans\n",
        "\n",
        "x_list = []\n",
        "y_list = []\n",
        "\n",
        "# 출력하고 싶은 generation을 추가합니다. \n",
        "# 예를 들어 generation 1,2,3,5,10을 출력하고 싶다면 pareto_list = [1,2,3,5,10]으로 설정합니다.\n",
        "pareto_list=[1]\n",
        "for i in pareto_list:\n",
        "  pf_copy = np.load('/content/drive/My Drive/SBSE/pareto_front/pf_g'+str(i)+'.npy')\n",
        "  pf_copy = pf_copy.tolist()\n",
        "\n",
        "  pareto_front = set()\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  for pf in pf_copy:\n",
        "    _, num, val = pf\n",
        "    x.append(float(val))\n",
        "    y.append(int(num))\n",
        "\n",
        "  x_list.append(x)\n",
        "  y_list.append(y)\n",
        "\n",
        "color = ['red', 'orange', 'chartreuse', 'green', 'cyan', 'purple', 'magenta', 'brown']\n",
        "mlp.style.use('seaborn-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(10,4))\n",
        "\n",
        "for gen, (x, y) in enumerate(zip(x_list, y_list)):\n",
        "  x.sort()\n",
        "  y.sort(reverse=True)\n",
        "  label = str(pareto_list[gen])\n",
        "  lw=10-gen*1\n",
        "  ls=['-','--','-.',':'][gen%4]\n",
        "  ax.plot(x,y, marker='o',label=label, drawstyle='steps-post', color=color[gen%8], linewidth = lw, alpha=1-gen*0.1)\n",
        "  #ax.plot(x, y, marker='o', linestyle='', label=label)\n",
        "\n",
        "\n",
        "ax.legend(loc='best')\n",
        "\n",
        "plt.legend(title='Generation',prop={'size': 12}, ncol=2)\n",
        "plt.title('Progress of Pareto front')\n",
        "plt.xlabel('Validation error',fontsize=10)\n",
        "plt.ylabel('Number of Parameters',fontsize=10)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.show()\n",
        "fig.savefig('/content/drive/My Drive/SBSE/pareto_front(1).svg', format='svg')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lP1wINfGYb2",
        "colab_type": "text"
      },
      "source": [
        "#Reference\n",
        "[1] Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution, Thomas Elsken et al. (ICLR 2019)\n",
        "\n",
        "[2] Fashion-MNIST dataset (https://github.com/zalandoresearch/fashion-mnist)"
      ]
    }
  ]
}